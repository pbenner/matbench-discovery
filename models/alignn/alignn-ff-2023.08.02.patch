diff --git a/alignn/data.py b/alignn/data.py
index 3c962c2..2e4be1a 100644
--- a/alignn/data.py
+++ b/alignn/data.py
@@ -174,8 +174,8 @@ def get_id_train_val_test(
     # full train/val test split
     # ids = ids[::-1]
     id_train = ids[:n_train]
-    id_val = ids[-(n_val + n_test) : -n_test]  # noqa:E203
-    id_test = ids[-n_test:]
+    id_val = ids[-(n_val + n_test) : -n_test]  if n_test > 0 else ids[-(n_val + n_test) :] # noqa:E203
+    id_test = ids[n_test:] if n_test > 0 else []
     return id_train, id_val, id_test
 
 
@@ -284,20 +284,35 @@ def get_train_val_loaders(
         print("Make sure all the DataLoader params are same.")
         print("This module is made for debugging only.")
         train_loader = torch.load(train_sample)
+        train_loader = DataLoader(
+            train_loader.dataset,
+            batch_size=batch_size,
+            shuffle=True,
+            collate_fn=train_loader.collate_fn,
+            drop_last=True,
+            num_workers=workers,
+            pin_memory=pin_memory,
+        )
         val_loader = torch.load(val_sample)
+        val_loader = DataLoader(
+            val_loader.dataset,
+            batch_size=batch_size,
+            shuffle=False,
+            collate_fn=val_loader.collate_fn,
+            drop_last=True,
+            num_workers=workers,
+            pin_memory=pin_memory,
+        )
         test_loader = torch.load(test_sample)
-        if train_loader.pin_memory != pin_memory:
-            train_loader.pin_memory = pin_memory
-        if test_loader.pin_memory != pin_memory:
-            test_loader.pin_memory = pin_memory
-        if val_loader.pin_memory != pin_memory:
-            val_loader.pin_memory = pin_memory
-        if train_loader.num_workers != workers:
-            train_loader.num_workers = workers
-        if test_loader.num_workers != workers:
-            test_loader.num_workers = workers
-        if val_loader.num_workers != workers:
-            val_loader.num_workers = workers
+        test_loader = DataLoader(
+            test_loader.dataset,
+            batch_size=batch_size,
+            shuffle=False,
+            collate_fn=test_loader.collate_fn,
+            drop_last=False,
+            num_workers=workers,
+            pin_memory=pin_memory,
+        )
         # print("train", len(train_loader.dataset))
         # print("val", len(val_loader.dataset))
         # print("test", len(test_loader.dataset))
@@ -489,7 +504,7 @@ def get_train_val_loaders(
             classification=classification_threshold is not None,
             output_dir=output_dir,
             tmp_name="val_data",
-        )
+        ) if len(dataset_val) > 0 else None
         test_data = get_torch_dataset(
             dataset=dataset_test,
             id_tag=id_tag,
@@ -507,7 +522,7 @@ def get_train_val_loaders(
             classification=classification_threshold is not None,
             output_dir=output_dir,
             tmp_name="test_data",
-        )
+        ) if len(dataset_test) > 0 else None
 
         collate_fn = train_data.collate
         # print("line_graph,line_dih_graph", line_graph, line_dih_graph)
@@ -527,7 +542,7 @@ def get_train_val_loaders(
 
         val_loader = DataLoader(
             val_data,
-            batch_size=batch_size,
+            batch_size=1,
             shuffle=False,
             collate_fn=collate_fn,
             drop_last=True,
@@ -543,14 +558,18 @@ def get_train_val_loaders(
             drop_last=False,
             num_workers=workers,
             pin_memory=pin_memory,
-        )
+        ) if len(dataset_test) > 0 else None
+
         if save_dataloader:
             torch.save(train_loader, train_sample)
-            torch.save(val_loader, val_sample)
-            torch.save(test_loader, test_sample)
+            if val_loader is not None:
+                torch.save(val_loader, val_sample)
+            if test_loader is not None:
+                torch.save(test_loader, test_sample)
+
     print("n_train:", len(train_loader.dataset))
-    print("n_val:", len(val_loader.dataset))
-    print("n_test:", len(test_loader.dataset))
+    print("n_val  :", len(  val_loader.dataset) if  val_loader is not None else 0)
+    print("n_test :", len( test_loader.dataset) if test_loader is not None else 0)
     return (
         train_loader,
         val_loader,
diff --git a/alignn/ff/ff.py b/alignn/ff/ff.py
index 34e6b21..38eaf28 100644
--- a/alignn/ff/ff.py
+++ b/alignn/ff/ff.py
@@ -339,6 +339,7 @@ class ForceField(object):
         force_multiplier=1.0,
         force_mult_natoms=False,
         batch_stress=True,
+        device=None,
     ):
         """Intialize class."""
         self.jarvis_atoms = jarvis_atoms
@@ -384,7 +385,7 @@ class ForceField(object):
                 force_multiplier=self.force_multiplier,
                 force_mult_natoms=self.force_mult_natoms,
                 batch_stress=self.batch_stress,
-                # device="cuda" if torch.cuda.is_available() else "cpu",
+                device=device,
             )
         )
 
@@ -1181,6 +1182,7 @@ def phonons(
     phonopy_bands_figname="phonopy_bands.png",
     # phonopy_dos_figname="phonopy_dos.png",
     write_fc=False,
+    device=None,
 ):
     """Make Phonon calculation setup."""
     calc = AlignnAtomwiseCalculator(
@@ -1188,6 +1190,7 @@ def phonons(
         force_mult_natoms=False,
         force_multiplier=1,
         stress_wt=-4800,
+        device=device,
     )
 
     from phonopy import Phonopy
@@ -1321,11 +1324,12 @@ def phonons3(
     model_filename="best_model.pt",
     on_relaxed_struct=False,
     dim=[2, 2, 2],
+    device=None,
 ):
     """Make Phonon3 calculation setup."""
     from phono3py import Phono3py
 
-    calc = AlignnAtomwiseCalculator(path=model_path)
+    calc = AlignnAtomwiseCalculator(path=model_path, device=device)
 
     # kpoints = Kpoints().kpath(atoms, line_density=line_density)
     # dim = get_supercell_dims(cvn, enforce_c_size=enforce_c_size)
@@ -1383,9 +1387,10 @@ def ase_phonon(
     filename="Atom_phonon.png",
     ev_file=None,
     model_path="",
+    device=None,
 ):
     """Get phonon bandstructure and DOS using ASE."""
-    calc = AlignnAtomwiseCalculator(path=model_path)
+    calc = AlignnAtomwiseCalculator(path=model_path, device=device)
     # Setup crystal and EMT calculator
     # atoms = bulk("Al", "fcc", a=4.05)
 
diff --git a/alignn/run_alignn_ff.py b/alignn/run_alignn_ff.py
index 8dfa791..8e9afd1 100644
--- a/alignn/run_alignn_ff.py
+++ b/alignn/run_alignn_ff.py
@@ -62,6 +62,10 @@ intf_line = (
 )
 parser.add_argument("--interface_info", default=None, help=intf_line)
 
+parser.add_argument(
+    "--device", default=None, help="set device for executing the model [e.g. cpu, cuda, cuda:2]"
+)
+
 if __name__ == "__main__":
     args = parser.parse_args(sys.argv[1:])
     model_path = args.model_path
@@ -72,6 +76,7 @@ if __name__ == "__main__":
     initial_temperature_K = float(args.initial_temperature_K)
     on_relaxed_struct_yn = args.on_relaxed_struct
     timestep = float(args.timestep)
+    device = args.device
     if on_relaxed_struct_yn.lower() == "yes":
         on_relaxed_struct = True
     else:
@@ -95,6 +100,7 @@ if __name__ == "__main__":
         ff = ForceField(
             jarvis_atoms=atoms,
             model_path=model_path,
+            device=device,
         )
         energy = ff.unrelaxed_atoms()
         print("Energy(eV)", energy)
@@ -102,6 +108,7 @@ if __name__ == "__main__":
         ff = ForceField(
             jarvis_atoms=atoms,
             model_path=model_path,
+            device=device,
         )
         opt, en, fs = ff.optimize_atoms()
         print("initial struct:")
@@ -115,6 +122,7 @@ if __name__ == "__main__":
             jarvis_atoms=atoms,
             model_path=model_path,
             timestep=timestep,
+            device=device,
         )
         nptt = ff.run_nvt_berendsen(
             steps=steps,
@@ -127,6 +135,7 @@ if __name__ == "__main__":
         ff = ForceField(
             jarvis_atoms=atoms,
             model_path=model_path,
+            device=device,
         )
         ev = ev_curve(
             atoms=atoms,
@@ -137,6 +146,7 @@ if __name__ == "__main__":
         ff = ForceField(
             jarvis_atoms=atoms,
             model_path=model_path,
+            device=device,
         )
         vac = vacancy_formation(
             atoms=atoms,
@@ -148,6 +158,7 @@ if __name__ == "__main__":
         ff = ForceField(
             jarvis_atoms=atoms,
             model_path=model_path,
+            device=device,
         )
         surf = surface_energy(
             atoms=atoms,
@@ -197,6 +208,7 @@ if __name__ == "__main__":
             jarvis_atoms=atoms,
             model_path=model_path,
             timestep=timestep,
+            device=device,
         )
         nptt = ff.run_npt_berendsen(
             steps=steps,
@@ -212,6 +224,7 @@ if __name__ == "__main__":
             jarvis_atoms=atoms,
             model_path=model_path,
             timestep=timestep,
+            device=device,
         )
         lang = ff.run_nvt_langevin(
             steps=steps,
@@ -228,6 +241,7 @@ if __name__ == "__main__":
             jarvis_atoms=atoms,
             model_path=model_path,
             timestep=timestep,
+            device=device,
         )
         vv = ff.run_nve_velocity_verlet(
             steps=steps, initial_temperature_K=initial_temperature_K
@@ -241,6 +255,7 @@ if __name__ == "__main__":
             jarvis_atoms=atoms,
             model_path=model_path,
             timestep=timestep,
+            device=device,
         )
         nptt = ff.run_npt_nose_hoover(
             steps=steps,
diff --git a/alignn/train.py b/alignn/train.py
index 1ce7928..1ce7738 100644
--- a/alignn/train.py
+++ b/alignn/train.py
@@ -70,11 +70,6 @@ warnings.filterwarnings("ignore", category=RuntimeWarning)
 # torch config
 torch.set_default_dtype(torch.float32)
 
-device = "cpu"
-if torch.cuda.is_available():
-    device = torch.device("cuda")
-
-
 def activated_output_transform(output):
     """Exponentiate output."""
     y_pred, y = output
@@ -87,8 +82,8 @@ def make_standard_scalar_and_pca(output):
     """Use standard scalar and PCS for multi-output data."""
     sc = pk.load(open(os.path.join(tmp_output_dir, "sc.pkl"), "rb"))
     y_pred, y = output
-    y_pred = torch.tensor(sc.transform(y_pred.cpu().numpy()), device=device)
-    y = torch.tensor(sc.transform(y.cpu().numpy()), device=device)
+    y_pred = torch.tensor(sc.transform(y_pred.cpu().numpy()), device=y_pred.device)
+    y = torch.tensor(sc.transform(y.cpu().numpy()), device=y.device)
     # pc = pk.load(open("pca.pkl", "rb"))
     # y_pred = torch.tensor(pc.transform(y_pred), device=device)
     # y = torch.tensor(pc.transform(y), device=device)
@@ -147,6 +142,7 @@ def train_dgl(
     # checkpoint_dir: Path = Path("./"),
     train_val_test_loaders=[],
     # log_tensorboard: bool = False,
+    device=None,
 ):
     """Training entry point for DGL networks.
 
@@ -180,6 +176,10 @@ def train_dgl(
     if config.random_seed is not None:
         deterministic = True
         ignite.utils.manual_seed(config.random_seed)
+    if device is None:
+        device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu"
+        )
 
     line_graph = False
     alignn_models = {
@@ -242,9 +242,7 @@ def train_dgl(
         val_loader = train_val_test_loaders[1]
         test_loader = train_val_test_loaders[2]
         prepare_batch = train_val_test_loaders[3]
-    device = "cpu"
-    if torch.cuda.is_available():
-        device = torch.device("cuda")
+
     if config.distributed:
         print(
             "Using Accelerator, currently experimental, use at your own risk."
@@ -947,13 +945,23 @@ def train_dgl(
             "lr_scheduler": scheduler,
             "trainer": trainer,
         }
+        if classification:
+            def cp_score(engine):
+                """Higher accuracy is better."""
+                return engine.state.metrics["accuracy"]
+        else:
+            def cp_score(engine):
+                """Lower MAE is better."""
+                return -engine.state.metrics["mae"]
+
         handler = Checkpoint(
             to_save,
             DiskSaver(checkpoint_dir, create_dir=True, require_empty=False),
             n_saved=2,
             global_step_transform=lambda *_: trainer.state.epoch,
+            score_function=cp_score,
         )
-        trainer.add_event_handler(Events.EPOCH_COMPLETED, handler)
+        evaluator.add_event_handler(Events.EPOCH_COMPLETED, handler)
     if config.progress:
         pbar = ProgressBar()
         pbar.attach(trainer, output_transform=lambda x: {"loss": x})
@@ -1024,13 +1032,13 @@ def train_dgl(
 
             def es_score(engine):
                 """Higher accuracy is better."""
-                engine.state.metrics["accuracy"]
+                return engine.state.metrics["accuracy"]
 
         else:
 
             def es_score(engine):
                 """Lower MAE is better."""
-                -engine.state.metrics["mae"]
+                return -engine.state.metrics["mae"]
 
         es_handler = EarlyStopping(
             patience=config.n_early_stopping,
@@ -1063,7 +1071,7 @@ def train_dgl(
         test_loss = evaluator.state.metrics["loss"]
         tb_logger.writer.add_hparams(config, {"hparam/test_loss": test_loss})
         tb_logger.close()
-    if config.write_predictions and classification:
+    if config.write_predictions and classification and test_loader is not None:
         net.eval()
         f = open(
             os.path.join(config.output_dir, "prediction_results_test_set.csv"),
@@ -1100,6 +1108,7 @@ def train_dgl(
         config.write_predictions
         and not classification
         and config.model.output_features > 1
+        and test_loader is not None
     ):
         net.eval()
         mem = []
@@ -1130,6 +1139,7 @@ def train_dgl(
         config.write_predictions
         and not classification
         and config.model.output_features == 1
+        and test_loader is not None
     ):
         net.eval()
         f = open(
diff --git a/alignn/train_folder.py b/alignn/train_folder.py
index 003c247..67bd8df 100644
--- a/alignn/train_folder.py
+++ b/alignn/train_folder.py
@@ -57,6 +57,9 @@ parser.add_argument(
     help="Folder to save outputs",
 )
 
+parser.add_argument(
+    "--device", default=None, help="set device for training the model [e.g. cpu, cuda, cuda:2]"
+)
 
 def train_for_folder(
     root_dir="examples/sample_data",
@@ -67,6 +70,7 @@ def train_for_folder(
     epochs=None,
     file_format="poscar",
     output_dir=None,
+    device=None,
 ):
     """Train for a folder."""
     # config_dat=os.path.join(root_dir,config_name)
@@ -185,6 +189,7 @@ def train_for_folder(
             test_loader,
             prepare_batch,
         ],
+        device=device,
     )
     t2 = time.time()
     print("Time taken (s):", t2 - t1)
@@ -203,4 +208,5 @@ if __name__ == "__main__":
         batch_size=(args.batch_size),
         epochs=(args.epochs),
         file_format=(args.file_format),
+        device=(args.device),
     )
diff --git a/alignn/train_folder_ff.py b/alignn/train_folder_ff.py
index 52e4c48..00eff87 100644
--- a/alignn/train_folder_ff.py
+++ b/alignn/train_folder_ff.py
@@ -16,11 +16,6 @@ from alignn.models.alignn_atomwise import ALIGNNAtomWise, ALIGNNAtomWiseConfig
 import torch
 import time
 
-device = "cpu"
-if torch.cuda.is_available():
-    device = torch.device("cuda")
-
-
 parser = argparse.ArgumentParser(
     description="Atomistic Line Graph Neural Network"
 )
@@ -106,6 +101,9 @@ parser.add_argument(
     help="Checkpoint file path for model",
 )
 
+parser.add_argument(
+    "--device", default=None, help="set device for training the model [e.g. cpu, cuda, cuda:2]"
+)
 
 def train_for_folder(
     root_dir="examples/sample_data",
@@ -124,6 +122,7 @@ def train_for_folder(
     # subtract_mean=False,
     # normalize_with_natoms=False,
     output_dir=None,
+    device=None,
 ):
     """Train for a folder."""
     dat = loadjson(os.path.join(root_dir, "id_prop.json"))
@@ -311,6 +310,7 @@ def train_for_folder(
             test_loader,
             prepare_batch,
         ],
+        device=device,
     )
     t2 = time.time()
     print("Time taken (s)", t2 - t1)
@@ -337,4 +337,5 @@ if __name__ == "__main__":
         # subtract_mean=(args.subtract_mean),
         # normalize_with_natoms=(args.normalize_with_natoms),
         file_format=(args.file_format),
+        device=(args.device),
     )
